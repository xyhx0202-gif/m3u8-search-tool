#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
M3U8æœç´¢å·¥å…· - ç®€åŒ–Webç‰ˆï¼ˆä½¿ç”¨å†…ç½®http.serverï¼‰
"""

import http.server
import socketserver
import socket
import urllib.request
import urllib.parse
import re
import ssl
import json
import os
import base64
from urllib.parse import parse_qs, urlparse

# å…¨å±€å˜é‡ç”¨äºAPIè¯·æ±‚é¢‘ç‡é™åˆ¶
api_requests = {}

# ç«¯å£é…ç½®
PORT = 8888
# APIå¯†é’¥ç”¨äºéªŒè¯è¯·æ±‚
API_SECRET_KEY = "Xm3U8V1d30p1D"

def generate_api_token():
    """ç”ŸæˆAPIè°ƒç”¨çš„ä¸´æ—¶token"""
    import time
    timestamp = str(int(time.time()))
    # ç®€å•çš„tokenç”Ÿæˆï¼šæ—¶é—´æˆ³ + å¯†é’¥çš„éƒ¨åˆ†å­—ç¬¦æ‹¼æ¥
    token = timestamp + "_" + API_SECRET_KEY[2:8]
    return token

def validate_api_token(token):
    """éªŒè¯API tokenæ˜¯å¦æœ‰æ•ˆ"""
    try:
        import time
        if not token or '_' not in token:
            return False
        
        timestamp_str, secret_part = token.split('_')
        timestamp = int(timestamp_str)
        current_time = int(time.time())
        
        # éªŒè¯æ—¶é—´æˆ³æ˜¯å¦åœ¨5åˆ†é’Ÿå†…
        if abs(current_time - timestamp) > 300:
            return False
        
        # éªŒè¯å¯†é’¥éƒ¨åˆ†
        return secret_part == API_SECRET_KEY[2:8]
    except Exception:
        return False

def check_rate_limit(client_ip):
    """æ£€æŸ¥APIè¯·æ±‚é¢‘ç‡é™åˆ¶"""
    import time
    current_time = time.time()
    
    if client_ip not in api_requests:
        api_requests[client_ip] = []
    
    # æ¸…ç†5åˆ†é’Ÿå‰çš„è¯·æ±‚è®°å½•
    api_requests[client_ip] = [t for t in api_requests[client_ip] if current_time - t < 300]
    
    # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é¢‘ç‡é™åˆ¶ï¼ˆæ¯åˆ†é’Ÿæœ€å¤š30æ¬¡è¯·æ±‚ï¼‰
    recent_requests = [t for t in api_requests[client_ip] if current_time - t < 60]
    if len(recent_requests) >= 30:
        return False
    
    # è®°å½•æœ¬æ¬¡è¯·æ±‚
    api_requests[client_ip].append(current_time)
    return True

def encrypt_m3u8_url(url):
    """åŠ å¯†M3U8åœ°å€ï¼ˆç®€å•å®ç°ï¼‰"""
    # è¿™é‡Œåªæ˜¯ä¸€ä¸ªç®€å•çš„æ¼”ç¤ºï¼Œå®é™…é¡¹ç›®ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„åŠ å¯†ç®—æ³•
    import base64
    return base64.b64encode(url.encode()).decode()

def decrypt_m3u8_url(encrypted_url):
    """ç®€åŒ–çš„M3U8åœ°å€å¤„ç†"""
    # ä¸å†è¿›è¡Œè§£å¯†ï¼Œç›´æ¥è¿”å›åŸå§‹URL
    return encrypted_url

class M3U8SearchHandler(http.server.SimpleHTTPRequestHandler):
    def do_GET(self):
        """å¤„ç†GETè¯·æ±‚"""
        if self.path == '/' or self.path == '/index.html':
            # è¿”å›ä¸»é¡µ
            self.send_response(200)
            self.send_header('Content-type', 'text/html; charset=utf-8')
            self.end_headers()
            
            # è¯»å–ç°æœ‰çš„HTMLæ–‡ä»¶
            try:
                with open('index_simple.html', 'r', encoding='utf-8') as f:
                    html_content = f.read()
                self.wfile.write(html_content.encode('utf-8'))
            except FileNotFoundError:
                # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºåŸºç¡€æ–‡ä»¶
                create_simple_html()
                with open('index_simple.html', 'r', encoding='utf-8') as f:
                    html_content = f.read()
                self.wfile.write(html_content.encode('utf-8'))
            
        elif self.path.startswith('/api/') or self.path.startswith('/v1/'):
            # å¤„ç†APIè¯·æ±‚ï¼ˆæ”¯æŒåŸå§‹è·¯å¾„å’Œæ··æ·†è·¯å¾„ï¼‰
            self.handle_api_request()
        else:
            # é™æ€æ–‡ä»¶æœåŠ¡
            super().do_GET()
    
    def do_POST(self):
        """å¤„ç†POSTè¯·æ±‚"""
        if self.path.startswith('/api/') or self.path.startswith('/v1/'):
            self.handle_api_request()
        else:
            self.send_error(404, "File not found")
    
    def handle_api_request(self):
        """å¤„ç†APIè¯·æ±‚ï¼ˆæ·»åŠ æ··æ·†å’ŒéªŒè¯ï¼‰"""
        # 1. éªŒè¯è¯·æ±‚å¤´ä¸­çš„ç‰¹æ®Šæ ‡è¯†ï¼ˆæ”¯æŒå‰ç«¯ä½¿ç”¨çš„X-API-Keyï¼‰
        x_api_key = self.headers.get('X-API-Key')
        
        # æ”¯æŒä¸¤ç§è¯·æ±‚å¤´éªŒè¯æ–¹å¼
        if not x_api_key or x_api_key != 'm3u8_viewer_key':
            self.send_response(403)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            self.wfile.write(json.dumps({'error': 'Forbidden: Invalid API key'}).encode('utf-8'))
            return
        
        # 2. éªŒè¯API token
        api_token = self.headers.get('X-API-Token')
        if not validate_api_token(api_token):
            # è¿”å›æ–°çš„token
            new_token = generate_api_token()
            self.send_response(401)
            self.send_header('X-New-Token', new_token)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            self.wfile.write(json.dumps({'error': 'Token expired', 'new_token': new_token}).encode('utf-8'))
            return
        
        # 3. æ£€æŸ¥è¯·æ±‚é¢‘ç‡é™åˆ¶
        client_ip = self.client_address[0]
        if not check_rate_limit(client_ip):
            self.send_response(429)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            self.wfile.write(json.dumps({'error': 'Rate limit exceeded'}).encode('utf-8'))
            return
        
        # 4. è¯»å–è¯·æ±‚æ•°æ®ï¼ˆæ”¯æŒGETå’ŒPOSTï¼‰
        data = {}
        
        if self.command == 'GET':
            # å¤„ç†GETè¯·æ±‚çš„æŸ¥è¯¢å‚æ•°
            parsed_url = urlparse(self.path)
            query_params = parse_qs(parsed_url.query)
            
            # è½¬æ¢ä¸ºæ™®é€šå­—å…¸
            for key, values in query_params.items():
                data[key] = values[0] if len(values) == 1 else values
                
            # é’ˆå¯¹/v1/queryè·¯å¾„ï¼Œå°†qå‚æ•°è½¬æ¢ä¸ºvideo_name
            if self.path.startswith('/v1/query') and 'q' in data:
                data['video_name'] = data['q']
            # é’ˆå¯¹/v1/streamè·¯å¾„ï¼Œå°†idå‚æ•°è½¬æ¢ä¸ºvideo_id
            elif self.path.startswith('/v1/stream') and 'id' in data:
                data['video_id'] = data['id']
            # é’ˆå¯¹/v1/episodesè·¯å¾„ï¼Œå°†episodeå‚æ•°è½¬æ¢ä¸ºepisode_url
            elif self.path.startswith('/v1/episodes') and 'episode' in data:
                data['episode_url'] = data['episode']
        else:  # POSTè¯·æ±‚
            # å¤„ç†POSTè¯·æ±‚çš„JSONæ•°æ®
            if 'Content-Length' in self.headers:
                content_length = int(self.headers['Content-Length'])
                post_data = self.rfile.read(content_length).decode('utf-8')
                
                try:
                    data = json.loads(post_data)
                except:
                    data = {}
        
        # 5. å¤„ç†æ··æ·†çš„APIè·¯å¾„
        # æ”¯æŒæ··æ·†è·¯å¾„å’ŒåŸå§‹è·¯å¾„ï¼Œå¢åŠ çˆ¬è™«éš¾åº¦
        api_paths = {
            # åŸå§‹è·¯å¾„
            '/api/search': self.api_search,
            '/api/get_m3u8': self.api_get_m3u8,
            '/api/get_episode_m3u8': self.api_get_episode_m3u8,
            # æ··æ·†è·¯å¾„
            '/v1/query': self.api_search,
            '/v1/stream': self.api_get_m3u8,
            '/v1/episodes': self.api_get_episode_m3u8
        }
        
        # æå–è·¯å¾„éƒ¨åˆ†ï¼ˆå¿½ç•¥æŸ¥è¯¢å‚æ•°ï¼‰
        path_without_query = urlparse(self.path).path
        if path_without_query in api_paths:
            response = api_paths[path_without_query](data)
        else:
            response = {'error': 'API not found'}
        
        # æ·»åŠ å“åº”å¤´ï¼Œé˜²æ­¢ç®€å•çš„çˆ¬è™«æ£€æµ‹
        self.send_response(200)
        self.send_header('Content-type', 'application/json; charset=utf-8')
        self.send_header('X-Powered-By', 'Unknown-Server')
        self.send_header('Server', 'Custom-HTTP/1.1')
        self.send_header('Access-Control-Allow-Origin', '*')
        self.send_header('Access-Control-Allow-Headers', 'Content-Type, X-API-Token, X-Client-ID, X-App-Version')
        self.end_headers()
        self.wfile.write(json.dumps(response, ensure_ascii=False).encode('utf-8'))
    
    def setup_proxy(self):
        """è®¾ç½®ä»£ç†"""
        try:
            # å°è¯•å¤šç§ä»£ç†é…ç½®
            proxy_configs = [
                {'http': 'http://127.0.0.1:7897', 'https': 'https://127.0.0.1:7897'},
                {'http': 'http://127.0.0.1:7890', 'https': 'https://127.0.0.1:7890'},
                {'http': 'http://127.0.0.1:1080', 'https': 'https://127.0.0.1:1080'},
                {}  # æ— ä»£ç†
            ]
            
            for proxy_config in proxy_configs:
                try:
                    proxy_handler = urllib.request.ProxyHandler(proxy_config)
                    opener = urllib.request.build_opener(proxy_handler)
                    urllib.request.install_opener(opener)
                    
                    # æµ‹è¯•è¿æ¥
                    test_url = "https://www.baidu.com"
                    response = urllib.request.urlopen(test_url, timeout=5)
                    print(f"ä»£ç†è®¾ç½®æˆåŠŸ: {proxy_config}")
                    return
                except Exception as e:
                    print(f"ä»£ç†æµ‹è¯•å¤±è´¥ {proxy_config}: {e}")
                    continue
            
            print("æ‰€æœ‰ä»£ç†é…ç½®éƒ½å¤±è´¥ï¼Œä½¿ç”¨æ— ä»£ç†æ¨¡å¼")
            
        except Exception as e:
            print(f"ä»£ç†è®¾ç½®å¼‚å¸¸: {e}")
            # ä½¿ç”¨æ— ä»£ç†
            proxy_handler = urllib.request.ProxyHandler({})
            opener = urllib.request.build_opener(proxy_handler)
            urllib.request.install_opener(opener)
    
    def fetch_page_content(self, url):
        """è·å–é¡µé¢å†…å®¹"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
        }
        
        try:
            req = urllib.request.Request(url, headers=headers)
            response = urllib.request.urlopen(req, timeout=30)
            return response.read().decode('utf-8')
        except Exception as e:
            print(f"è·å–é¡µé¢å¤±è´¥: {e}")
            return None
    
    def search_video(self, video_name):
        """æœç´¢è§†é¢‘å¹¶è¿”å›æ’­æ”¾é¡µé¢URL"""
        encoded_name = urllib.parse.quote_plus(video_name, safe='')
        search_url = f"https://xiaoxintv.cc/index.php/vod/search.html?wd={encoded_name}&submit="
        
        html_content = self.fetch_page_content(search_url)
        if not html_content:
            return None
        
        pattern = r'<a[^>]*href="(/index\.php/vod/detail/id/(\d+)\.html)"[^>]*title="([^"]+)"[^>]*>'
        matches = re.findall(pattern, html_content)
        
        results = []
        for match in matches:
            href, video_id, title = match
            if video_name.lower() in title.lower():
                play_url = f"https://xiaoxintv.cc/index.php/vod/play/id/{video_id}/sid/1/nid/1.html"
                results.append({
                    'title': title.strip(),
                    'video_id': video_id,
                    'play_url': play_url
                })
        
        return results
    
    def extract_m3u8_from_play_page(self, play_url):
        """ä»æ’­æ”¾é¡µé¢æå–M3U8åœ°å€"""
        print(f"å¼€å§‹æå–M3U8åœ°å€ï¼Œæ’­æ”¾é¡µé¢: {play_url}")
        
        html_content = self.fetch_page_content(play_url)
        if not html_content:
            print(f"æ— æ³•è·å–é¡µé¢å†…å®¹: {play_url}")
            return []
        
        print(f"é¡µé¢å†…å®¹é•¿åº¦: {len(html_content)} å­—ç¬¦")
        
        # æ›´å…¨é¢çš„M3U8åœ°å€åŒ¹é…æ¨¡å¼
        patterns = [
            # åŸºç¡€URLåŒ¹é…
            r'"(https?://[^"]+\.m3u8[^"]*)"',
            r"'(https?://[^']+\.m3u8[^']*)'",
            r'(https?://[^\s"\'<>]+\.m3u8)',
            
            # å±æ€§åŒ¹é…
            r'url\s*[:=]\s*["\'](https?://[^"\']+\.m3u8)["\']',
            r'videoUrl\s*[:=]\s*["\'](https?://[^"\']+\.m3u8)["\']',
            r'src\s*[:=]\s*["\'](https?://[^"\']+\.m3u8)["\']',
            r'file\s*[:=]\s*["\'](https?://[^"\']+\.m3u8)["\']',
            r'video\s*[:=]\s*["\'](https?://[^"\']+\.m3u8)["\']',
            
            # æ’­æ”¾å™¨é…ç½®
            r'player\.setup\s*\([^)]*url\s*[:=]\s*["\'](https?://[^"\']+\.m3u8)["\']',
            r'new\s+Player\s*\([^)]*url\s*[:=]\s*["\'](https?://[^"\']+\.m3u8)["\']',
            
            # JSONæ ¼å¼
            r'"url"\s*:\s*["\'](https?://[^"\']+\.m3u8)["\']',
            r'"src"\s*:\s*["\'](https?://[^"\']+\.m3u8)["\']',
            r'"file"\s*:\s*["\'](https?://[^"\']+\.m3u8)["\']',
            
            # ç›¸å¯¹è·¯å¾„ï¼ˆéœ€è¦æ‹¼æ¥å®Œæ•´URLï¼‰
            r'"(//[^"]+\.m3u8[^"]*)"',
            r"'(//[^']+\.m3u8[^']*)'",
            r'(//[^\s"\'<>]+\.m3u8)',
            
            # æ›´å®½æ¾çš„åŒ¹é…æ¨¡å¼
            r'[\"\'](https?://[^\"\']*?\.m3u8[^\"\']*?)[\"\']',
            r'(https?://[^\s<>]*?\.m3u8[^\s<>]*)',
            
            # åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„URL
            r'[\"\'](https?://[^\"\']*?m3u8[^\"\']*?)[\"\']',
            r'(https?://[^\s<>]*?m3u8[^\s<>]*)',
            
            # é’ˆå¯¹è¯¥ç½‘ç«™çš„ç‰¹æ®Šæ¨¡å¼ï¼šJavaScriptå˜é‡ä¸­çš„JSONæ ¼å¼
            r'var\s+player_[^=]*=\s*{[^}]*"url"\s*:\s*"([^"]+\.m3u8[^"]*)"',
            r'var\s+player_[^=]*=\s*{[^}]*"url"\s*:\s*\'([^\']+\.m3u8[^\']*)\'',
            r'"url"\s*:\s*"([^"]+\.m3u8[^"]*)"[^}]*}',
            r'"url"\s*:\s*\'([^\']+\.m3u8[^\']*)\'[^}]*}'
        ]
        
        m3u8_urls = []
        for i, pattern in enumerate(patterns):
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            if matches:
                print(f"æ¨¡å¼{i+1}åŒ¹é…åˆ°M3U8åœ°å€: {matches}")
                # å¤„ç†è½¬ä¹‰å­—ç¬¦
                processed_matches = []
                for match in matches:
                    # å¤„ç†JavaScriptä¸­çš„è½¬ä¹‰å­—ç¬¦
                    processed_url = match.replace('\\/', '/').replace('\\"', '"')
                    processed_matches.append(processed_url)
                m3u8_urls.extend(processed_matches)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾JavaScriptä¸­çš„M3U8åœ°å€
        if not m3u8_urls:
            print("å°è¯•åœ¨JavaScriptä¸­æŸ¥æ‰¾M3U8åœ°å€...")
            js_patterns = [
                r'var\s+[^=]*\s*=\s*["\'](https?://[^"\']+\.m3u8)["\']',
                r'let\s+[^=]*\s*=\s*["\'](https?://[^"\']+\.m3u8)["\']',
                r'const\s+[^=]*\s*=\s*["\'](https?://[^"\']+\.m3u8)["\']',
                r'window\.[^=]*\s*=\s*["\'](https?://[^"\']+\.m3u8)["\']',
                
                # æ›´å®½æ¾çš„JavaScriptåŒ¹é…
                r'var\s+[^=]*\s*=\s*["\'](https?://[^"\']*?\.m3u8)["\']',
                r'=[\s]*["\'](https?://[^"\']*?\.m3u8)["\']',
                r'url[\s]*[=:][\s]*["\'](https?://[^"\']*?\.m3u8)["\']'
            ]
            for pattern in js_patterns:
                matches = re.findall(pattern, html_content, re.IGNORECASE)
                if matches:
                    print(f"JavaScriptæ¨¡å¼åŒ¹é…åˆ°M3U8åœ°å€: {matches}")
                    m3u8_urls.extend(matches)
        
        # å¤„ç†ç›¸å¯¹è·¯å¾„
        processed_urls = []
        for url in m3u8_urls:
            if url.startswith('//'):
                # å°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºå®Œæ•´URL
                full_url = 'https:' + url
                processed_urls.append(full_url)
                print(f"ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºå®Œæ•´URL: {url} -> {full_url}")
            else:
                processed_urls.append(url)
        
        # å»é‡å¹¶è¿‡æ»¤æ— æ•ˆåœ°å€
        m3u8_urls = list(set(processed_urls))
        m3u8_urls = [url for url in m3u8_urls if 'm3u8' in url.lower()]
        
        print(f"æœ€ç»ˆæ‰¾åˆ°çš„M3U8åœ°å€æ•°é‡: {len(m3u8_urls)}")
        if m3u8_urls:
            print(f"M3U8åœ°å€åˆ—è¡¨: {m3u8_urls}")
        else:
            print("æœªæ‰¾åˆ°M3U8åœ°å€ï¼Œå°è¯•æŸ¥æ‰¾å…¶ä»–è§†é¢‘æ ¼å¼...")
            # æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„è§†é¢‘æ ¼å¼
            video_patterns = [
                r'"(https?://[^"]+\.mp4[^"]*)"',
                r'"(https?://[^"]+\.ts[^"]*)"',
                r'"(https?://[^"]+\.flv[^"]*)"'
            ]
            for pattern in video_patterns:
                matches = re.findall(pattern, html_content, re.IGNORECASE)
                if matches:
                    print(f"æ‰¾åˆ°å…¶ä»–è§†é¢‘æ ¼å¼: {matches}")
        
        return m3u8_urls
    
    def get_video_info(self, video_id):
        """è·å–è§†é¢‘çš„è¯¦ç»†ä¿¡æ¯ï¼ˆå°é¢ã€ç®€ä»‹ç­‰ï¼‰"""
        detail_url = f"https://xiaoxintv.cc/index.php/vod/detail/id/{video_id}.html"
        html_content = self.fetch_page_content(detail_url)
        
        if not html_content:
            print(f"æ— æ³•è·å–è¯¦æƒ…é¡µé¢å†…å®¹: {detail_url}")
            return {'cover': '', 'description': '', 'episodes': []}
        
        print(f"è¯¦æƒ…é¡µé¢å†…å®¹é•¿åº¦: {len(html_content)} å­—ç¬¦")
        
        # æå–å°é¢å›¾ç‰‡ï¼ˆä¼˜å…ˆJPGæ ¼å¼ï¼‰
        cover_url = ''
        cover_patterns = [
            # ä¼˜å…ˆåŒ¹é…JPGæ ¼å¼çš„å›¾ç‰‡
            r'<img[^>]*src="([^"]+\.jpg)"[^>]*alt="[^"]*"[^>]*>',
            r'<img[^>]*src="([^"]+\.jpeg)"[^>]*alt="[^"]*"[^>]*>',
            r'<img[^>]*data-original="([^"]+\.jpg)"[^>]*>',
            r'<img[^>]*data-original="([^"]+\.jpeg)"[^>]*>',
            r'background-image:\s*url\(["\']?([^"\']+\.jpg)["\']?\)',
            r'background-image:\s*url\(["\']?([^"\']+\.jpeg)["\']?\)',
            
            # å…¶ä»–æ ¼å¼çš„å›¾ç‰‡ï¼ˆä½œä¸ºå¤‡é€‰ï¼‰
            r'<img[^>]*src="([^"]+\.png)"[^>]*alt="[^"]*"[^>]*>',
            r'<img[^>]*src="([^"]+)"[^>]*class="stui-vodlist__thumb[^>]*>',
            r'<img[^>]*src="([^"]+)"[^>]*class="vod-pic[^>]*>',
            r'<img[^>]*src="([^"]+)"[^>]*alt="[^"]*"[^>]*class="[^"]*thumb[^"]*"[^>]*>',
            r'<img[^>]*src="([^"]+)"[^>]*alt="[^"]*"[^>]*class="[^"]*cover[^"]*"[^>]*>',
            r'<img[^>]*src="([^"]+)"[^>]*data-original="([^"]+)"[^>]*>',
            r'<img[^>]*data-original="([^"]+)"[^>]*>',
            r'background-image:\s*url\(["\']?([^"\')]+)["\']?\)',
            r'<img[^>]*src="([^"]+)"[^>]*style="[^"]*background[^"]*"[^>]*>',
            r'<img[^>]*src="([^"]+)"[^>]*width="[0-9]+"[^>]*height="[0-9]+"[^>]*>',
            r'<img[^>]*src="([^"]+)"[^>]*title="[^"]*"[^>]*>',
        ]
        
        for pattern in cover_patterns:
            cover_match = re.search(pattern, html_content, re.DOTALL | re.IGNORECASE)
            if cover_match:
                # è·å–ç¬¬ä¸€ä¸ªåŒ¹é…åˆ°çš„ç»„
                cover_url = cover_match.group(1)
                if not cover_url.startswith('http'):
                    # å¤„ç†ç›¸å¯¹è·¯å¾„
                    if cover_url.startswith('//'):
                        cover_url = 'https:' + cover_url
                    else:
                        cover_url = f"https://xiaoxintv.cc{cover_url}"
                print(f"æ‰¾åˆ°å°é¢å›¾ç‰‡: {cover_url}")
                
                # å¦‚æœæ˜¯JPGæ ¼å¼ï¼Œä¼˜å…ˆä½¿ç”¨
                if cover_url.lower().endswith(('.jpg', '.jpeg')):
                    break
        
        # æå–å®Œæ•´çš„è§†é¢‘ä¿¡æ¯ï¼ˆä¸»æ¼”ã€å¯¼æ¼”ã€ç®€ä»‹ç­‰ï¼‰
        description = ''
        
        # æå–å¯¼æ¼”ä¿¡æ¯
        director = ''
        director_patterns = [
            r'å¯¼æ¼”[ï¼š:]([^<]+)',
            r'<span[^>]*>å¯¼æ¼”[ï¼š:]<[^>]*>([^<]+)</',
            r'å¯¼æ¼”[ï¼š:]<[^>]*>([^<]+)</',
            r'<p[^>]*>å¯¼æ¼”[ï¼š:]([^<]+)</p>',
        ]
        
        for pattern in director_patterns:
            director_match = re.search(pattern, html_content, re.DOTALL | re.IGNORECASE)
            if director_match:
                director = re.sub(r'<[^>]+>', '', director_match.group(1)).strip()
                if director:
                    print(f"æ‰¾åˆ°å¯¼æ¼”ä¿¡æ¯: {director}")
                    break
        
        # æå–ä¸»æ¼”ä¿¡æ¯
        actors = ''
        actor_patterns = [
            r'ä¸»æ¼”[ï¼š:]([^<]+)',
            r'<span[^>]*>ä¸»æ¼”[ï¼š:]<[^>]*>([^<]+)</',
            r'ä¸»æ¼”[ï¼š:]<[^>]*>([^<]+)</',
            r'<p[^>]*>ä¸»æ¼”[ï¼š:]([^<]+)</p>',
        ]
        
        for pattern in actor_patterns:
            actor_match = re.search(pattern, html_content, re.DOTALL | re.IGNORECASE)
            if actor_match:
                actors = re.sub(r'<[^>]+>', '', actor_match.group(1)).strip()
                if actors:
                    print(f"æ‰¾åˆ°ä¸»æ¼”ä¿¡æ¯: {actors}")
                    break
        
        # æå–è¯¦ç»†ç®€ä»‹
        detail_desc = ''
        desc_patterns = [
            r'<div[^>]*class="stui-content__detail[^>]*>.*?<p[^>]*>(.*?)</p>',
            r'<div[^>]*class="vod-content[^>]*>.*?<p[^>]*>(.*?)</p>',
            r'<div[^>]*class="content[^>]*>.*?<p[^>]*>(.*?)</p>',
            r'<p[^>]*class="data[^>]*>(.*?)</p>',
            r'<div[^>]*class="detail[^>]*>.*?<p[^>]*>(.*?)</p>',
            r'<div[^>]*class="intro[^>]*>.*?<p[^>]*>(.*?)</p>',
            r'<div[^>]*class="description[^>]*>.*?<p[^>]*>(.*?)</p>',
            r'<div[^>]*class="summary[^>]*>.*?<p[^>]*>(.*?)</p>',
            r'<div[^>]*id="detail[^>]*>.*?<p[^>]*>(.*?)</p>',
            r'<div[^>]*id="intro[^>]*>.*?<p[^>]*>(.*?)</p>',
            r'<div[^>]*id="description[^>]*>.*?<p[^>]*>(.*?)</p>',
            r'<div[^>]*id="summary[^>]*>.*?<p[^>]*>(.*?)</p>',
            r'<p[^>]*class="intro[^>]*>(.*?)</p>',
            r'<p[^>]*class="description[^>]*>(.*?)</p>',
            r'<p[^>]*class="summary[^>]*>(.*?)</p>',
        ]
        
        for pattern in desc_patterns:
            desc_match = re.search(pattern, html_content, re.DOTALL | re.IGNORECASE)
            if desc_match:
                detail_desc = re.sub(r'<[^>]+>', '', desc_match.group(1)).strip()
                if len(detail_desc) > 0:
                    print(f"æ‰¾åˆ°è¯¦ç»†ç®€ä»‹: {detail_desc[:100]}...")
                    break
        
        # ç»„åˆå®Œæ•´çš„ç®€ä»‹ä¿¡æ¯
        description_parts = []
        if director:
            description_parts.append(f"å¯¼æ¼”: {director}")
        if actors:
            description_parts.append(f"ä¸»æ¼”: {actors}")
        if detail_desc:
            description_parts.append(f"ç®€ä»‹: {detail_desc}")
        
        if not description_parts:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¯¦ç»†ä¿¡æ¯ï¼Œä½¿ç”¨åŸæ¥çš„ç®€ä»‹æå–æ–¹å¼
            for pattern in desc_patterns:
                desc_match = re.search(pattern, html_content, re.DOTALL | re.IGNORECASE)
                if desc_match:
                    description = re.sub(r'<[^>]+>', '', desc_match.group(1)).strip()
                    if len(description) > 0:
                        print(f"æ‰¾åˆ°è§†é¢‘ç®€ä»‹: {description[:100]}...")
                        break
        else:
            description = '\\n'.join(description_parts)
        
        episodes = []
        
        # å¤šç§é›†æ•°åˆ—è¡¨åŒ¹é…æ¨¡å¼
        play_list_patterns = [
            r'<ul[^>]*class="stui-content__playlist[^>]*>(.*?)</ul>',
            r'<ul[^>]*class="playlist[^>]*>(.*?)</ul>',
            r'<div[^>]*class="playlist[^>]*>(.*?)</div>',
            r'<div[^>]*class="stui-content__playlist[^>]*>(.*?)</div>'
        ]
        
        play_list_html = None
        for pattern in play_list_patterns:
            play_list_match = re.search(pattern, html_content, re.DOTALL | re.IGNORECASE)
            if play_list_match:
                play_list_html = play_list_match.group(1)
                print(f"æ‰¾åˆ°é›†æ•°åˆ—è¡¨ï¼Œä½¿ç”¨æ¨¡å¼: {pattern[:50]}...")
                break
        
        if not play_list_html:
            print("æœªæ‰¾åˆ°é›†æ•°åˆ—è¡¨ï¼Œå°è¯•ç›´æ¥åœ¨æ•´ä¸ªé¡µé¢ä¸­æŸ¥æ‰¾é›†æ•°é“¾æ¥")
            play_list_html = html_content
        
        # å¤šç§é›†æ•°é“¾æ¥åŒ¹é…æ¨¡å¼
        episode_patterns = [
            r'<a[^>]*href="(/index\.php/vod/play/id/\d+/sid/(\d+)/nid/(\d+)\.html)"[^>]*>(.*?)</a>',
            r'<a[^>]*href="(/vod/play/id/\d+/sid/(\d+)/nid/(\d+)\.html)"[^>]*>(.*?)</a>',
            r'<a[^>]*href="(/play/\d+/\d+/\d+)"[^>]*>(.*?)</a>',
            r'<a[^>]*href="(https?://[^"]+/index\.php/vod/play/id/\d+/sid/(\d+)/nid/(\d+)\.html)"[^>]*>(.*?)</a>'
        ]
        
        for pattern in episode_patterns:
            episode_matches = re.findall(pattern, play_list_html, re.DOTALL | re.IGNORECASE)
            if episode_matches:
                print(f"æ‰¾åˆ° {len(episode_matches)} ä¸ªé›†æ•°ï¼Œä½¿ç”¨æ¨¡å¼: {pattern[:50]}...")
                
                for match in episode_matches:
                    if len(match) == 4:
                        href, sid, nid, title = match
                        # ä»HTMLä¸­æå–çº¯æ–‡æœ¬æ ‡é¢˜
                        clean_title = re.sub(r'<[^>]+>', '', title).strip()
                        if not clean_title:
                            clean_title = f"ç¬¬{nid}é›†"
                        
                        if not href.startswith('http'):
                            full_url = f"https://xiaoxintv.cc{href}"
                        else:
                            full_url = href
                        episodes.append({
                            'sid': sid,
                            'nid': nid,
                            'title': clean_title,
                            'url': full_url
                        })
                    elif len(match) == 2:
                        href, title = match
                        # ä»HTMLä¸­æå–çº¯æ–‡æœ¬æ ‡é¢˜
                        clean_title = re.sub(r'<[^>]+>', '', title).strip()
                        if not clean_title:
                            # ä»URLä¸­æå–sidå’Œnid
                            sid_nid_match = re.search(r'/sid/(\d+)/nid/(\d+)', href)
                            if sid_nid_match:
                                sid, nid = sid_nid_match.groups()
                                clean_title = f"ç¬¬{nid}é›†"
                        
                        # ä»URLä¸­æå–sidå’Œnid
                        sid_nid_match = re.search(r'/sid/(\d+)/nid/(\d+)', href)
                        if sid_nid_match:
                            sid, nid = sid_nid_match.groups()
                            if not href.startswith('http'):
                                full_url = f"https://xiaoxintv.cc{href}"
                            else:
                                full_url = href
                            episodes.append({
                                'sid': sid,
                                'nid': nid,
                                'title': clean_title,
                                'url': full_url
                            })
                
                if episodes:
                    break
        
        # å»é‡
        unique_episodes = []
        seen_urls = set()
        for episode in episodes:
            if episode['url'] not in seen_urls:
                unique_episodes.append(episode)
                seen_urls.add(episode['url'])
        
        print(f"æœ€ç»ˆæå–åˆ° {len(unique_episodes)} ä¸ªé›†æ•°")
        
        return {
            'cover': cover_url,
            'description': description,
            'episodes': unique_episodes
        }
    
    def api_search(self, data):
        """æœç´¢è§†é¢‘API"""
        video_name = data.get('video_name', '').strip()
        
        if not video_name:
            return {'error': 'è¯·è¾“å…¥è§†é¢‘åç§°'}
        
        self.setup_proxy()
        ssl._create_default_https_context = ssl._create_unverified_context
        
        results = self.search_video(video_name)
        
        if not results:
            return {'error': 'æœªæ‰¾åˆ°ç›¸å…³è§†é¢‘'}
        
        return {'success': True, 'results': results}
    
    def api_get_m3u8(self, data):
        """è·å–M3U8åœ°å€API"""
        video_id = data.get('video_id')
        play_url = data.get('play_url')
        
        if not play_url:
            return {'error': 'ç¼ºå°‘æ’­æ”¾é¡µé¢URL'}
        
        self.setup_proxy()
        ssl._create_default_https_context = ssl._create_unverified_context
        
        m3u8_urls = self.extract_m3u8_from_play_page(play_url)
        
        if not m3u8_urls:
            return {'error': 'æœªæ‰¾åˆ°M3U8åœ°å€'}
        
        video_info = {'cover': '', 'description': '', 'episodes': []}
        if video_id:
            video_info = self.get_video_info(video_id)
        
        # åŠ å¯†M3U8åœ°å€
        encrypted_m3u8_url = encrypt_m3u8_url(m3u8_urls[0])
        
        return {
            'success': True,
            'm3u8_url': encrypted_m3u8_url,
            'cover': video_info['cover'],
            'description': video_info['description'],
            'episodes': video_info['episodes']
        }
    
    def api_get_episode_m3u8(self, data):
        """è·å–æŒ‡å®šé›†æ•°çš„M3U8åœ°å€"""
        episode_url = data.get('episode_url')
        
        if not episode_url:
            return {'error': 'ç¼ºå°‘é›†æ•°URL'}
        
        self.setup_proxy()
        ssl._create_default_https_context = ssl._create_unverified_context
        
        m3u8_urls = self.extract_m3u8_from_play_page(episode_url)
        
        if not m3u8_urls:
            return {'error': 'æœªæ‰¾åˆ°M3U8åœ°å€'}
        
        # åŠ å¯†M3U8åœ°å€
        encrypted_m3u8_url = encrypt_m3u8_url(m3u8_urls[0])
        return {'success': True, 'm3u8_url': encrypted_m3u8_url}

def main():
    """ä¸»å‡½æ•°"""
    print("æ­£åœ¨åˆå§‹åŒ–æœåŠ¡å™¨...")
    # åˆ›å»ºç®€åŒ–çš„HTMLé¡µé¢
    create_simple_html()
    
    # è·å–ç«¯å£é…ç½®ï¼Œæ”¯æŒç¯å¢ƒå˜é‡é…ç½®ï¼ˆç”¨äºCloudflare Pageséƒ¨ç½²ç¯å¢ƒï¼‰
    port = int(os.environ.get('PORT', PORT))
    print(f"ä½¿ç”¨ç«¯å£: {port}")
    
    try:
        # åˆ›å»ºä¸€ä¸ªå¯ä»¥é‡ç”¨åœ°å€çš„æœåŠ¡å™¨ç±»
        socketserver.TCPServer.allow_reuse_address = True
        
        server_address = ("", port)
        print(f"å‡†å¤‡åˆ›å»ºæœåŠ¡å™¨å®ä¾‹ï¼Œåœ°å€: {server_address}")
        
        # åˆ›å»ºæœåŠ¡å™¨å®ä¾‹
        with socketserver.TCPServer(server_address, M3U8SearchHandler) as httpd:
            # è®¾ç½®è¯·æ±‚è¶…æ—¶ï¼ˆæé«˜ç¨³å®šæ€§ï¼‰
            httpd.timeout = 60
            
            print(f"ğŸš€ M3U8æœç´¢å·¥å…·å·²å¯åŠ¨")
            print(f"ğŸ“± è®¿é—®åœ°å€: http://localhost:{port}")
            print(f"ğŸŒ Cloudflare Pages éƒ¨ç½²åœ°å€: https://m3u8-search-tool.pages.dev/")
            print(f"ğŸ’¡ æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨")
            print("æœåŠ¡å™¨å¼€å§‹ç›‘å¬è¯·æ±‚...")
            
            # å¯åŠ¨æœåŠ¡å™¨
            try:
                httpd.serve_forever()
            except KeyboardInterrupt:
                print("\nğŸ‘‹ æœåŠ¡å™¨å·²åœæ­¢")
            except Exception as e:
                print(f"æœåŠ¡å™¨è¿è¡Œé”™è¯¯: {str(e)}")
                httpd.server_close()
    except Exception as e:
        print(f"æœåŠ¡å™¨åˆå§‹åŒ–é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

def create_simple_html():
    """åˆ›å»ºç®€åŒ–çš„HTMLé¡µé¢ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™ä¸å†è¦†ç›–
    if os.path.exists('index_simple.html'):
        print("index_simple.html æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
        return
    
    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºåŸºç¡€HTMLç»“æ„
    html_content = f'''<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>M3U8è§†é¢‘æœç´¢å·¥å…·</title>
    <script>
        // å…¨å±€é…ç½®
        const API_BASE_URL = 'http://localhost:{PORT}';
    </script>
</head>
<body>
    <h1>M3U8è§†é¢‘æœç´¢å·¥å…·</h1>
    <p>è¯·ç¡®ä¿ index_simple.html æ–‡ä»¶å­˜åœ¨</p>
</body>
</html>'''
    
    with open('index_simple.html', 'w', encoding='utf-8') as f:
        f.write(html_content)
    print("å·²åˆ›å»ºåŸºç¡€çš„ index_simple.html æ–‡ä»¶")

if __name__ == "__main__":
    main()